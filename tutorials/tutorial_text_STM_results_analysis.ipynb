{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in all the required files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_df = pd.read_csv('example_data/example_CM_inverter_records_with_embeddings.csv')\n",
    "# need to parse the embedding from a string into a list of tuples for each row\n",
    "CM_df['FullDescEmbeds'] = CM_df['FullDescEmbeds'].apply(lambda embed :\n",
    "                                                        [tuple([int(num) for num in pair.split(', ')])\n",
    "                                                        for pair in embed[2:-2].split('), (')]\n",
    "                                                        )\n",
    "\n",
    "with open('fitted_models/corpus_dictionary.pkl', 'rb') as file:\n",
    "    corpus_dictionary = pickle.load(file)\n",
    "\n",
    "lda_models = {}\n",
    "for k in [5, 10, 15, 20, 25, 30]:\n",
    "    with open(f'fitted_models/stm_lda_model_k{k}.pkl', 'rb') as file:\n",
    "        lda_models[k] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, for $k=30$, we can pull out a `dict` containing the terms associated with each topic. They are sorted from the highest probability to the lowest. The `dict` has format: `topic_terms[topic_id] = (term, probability)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('inverter', 0.056787804), ('fault', 0.037120067), ('ground', 0.03252461), ('offline', 0.022443272), ('cb', 0.018650837)]\n"
     ]
    }
   ],
   "source": [
    "k = 30\n",
    "topic_terms = {}\n",
    "for topic_id in range(k):\n",
    "    topic_term_embeds = lda_models[k].get_topic_terms(topic_id, topn=len(corpus_dictionary))\n",
    "    topic_terms[topic_id] = [(corpus_dictionary[embed], prob) if prob > 1e-2 else (corpus_dictionary[embed], 0)\n",
    "                                    for (embed, prob) in topic_term_embeds]\n",
    "\n",
    "# print the top 5 terms for topic_id 0\n",
    "print(topic_terms[0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigating correlations between topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can look at is how correlated different topics are. We can do this by getting the topic probabilities associated with each document and finding the Spearman (rank) correlation coefficient for a given pair of topics.\n",
    "<br><br>\n",
    "LDA assigns nonzero probabilities for every topic to each document. Therefore, in the code below, probabilities belows a cutoff value (`1e-2`) are considered to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agmoore\\AppData\\Local\\Temp\\ipykernel_7660\\81315066.py:16: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  correlation = spearmanr(topic_probs[topic_1], topic_probs[topic_2]).statistic\n"
     ]
    }
   ],
   "source": [
    "# for each document, get the probability of each topic\n",
    "document_topic_probs = [{topic_id : prob if prob > 1e-2 else 0\n",
    "                            for (topic_id, prob) in lda_models[k].get_document_topics(doc, minimum_probability=0)}\n",
    "                                for doc in CM_df['FullDescEmbeds']]\n",
    "\n",
    "# scale up so the probabilities add to one\n",
    "document_topic_probs = [{topic_id : prob / sum(doc.values()) for (topic_id, prob) in doc.items()} for doc in document_topic_probs]\n",
    "\n",
    "# for each topic, list out the probabilities for each document\n",
    "topic_probs = {topic_id : [doc[topic_id] for doc in document_topic_probs] for topic_id in range(k)}\n",
    "\n",
    "# get all the pairwise correlations between topics\n",
    "topic_correlations = {}\n",
    "for topic_1 in range(k):\n",
    "    for topic_2 in (range(topic_1 + 1, k)):\n",
    "            correlation = spearmanr(topic_probs[topic_1], topic_probs[topic_2]).statistic\n",
    "            if correlation > -1: # filters out cases where the spearman coef is nan (when one of the arrays is all constants)\n",
    "                topic_correlations[topic_1, topic_2] = correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put the results into a `DataFrame` and look at the most highly correlated topics. The code below finds the 5 most correlated pairs of topics and prints the top 10 words for each topic, as well as their correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 21 top words: ['inverter', 'production', 'position', 'outage', 'found', 'offline', 'techdispatched', 'comms', 'inverters', 'closed']\n",
      "Topic 22 top words: ['inverter', 'inverters', 'site', 'communicating', 'set', 'techdispatched', 'communication', 'data', 'system', 'pf']\n",
      "Correlation: 0.232\n",
      "\n",
      "Topic 14 top words: ['inverter', 'outage', 'production', 'w', 'techdispatched', 'communicating', 'c', 'stopped', 'system', 'hmi']\n",
      "Topic 22 top words: ['inverter', 'inverters', 'site', 'communicating', 'set', 'techdispatched', 'communication', 'data', 'system', 'pf']\n",
      "Correlation: 0.173\n",
      "\n",
      "Topic 1 top words: ['inverters', 'faults', 'inverter', 'techdispatched', 'created', 'wo', 'outage', 'central', 'pf', 'b']\n",
      "Topic 24 top words: ['inverter', 'outage', 'b', 'technician', 'site', 'reset', 'back', 'e', 'communicating', 'c']\n",
      "Correlation: 0.156\n",
      "\n",
      "Topic 7 top words: ['inverter', 'hardware', 'replaced', 'offline', 'dc', 'replacement', 'due', 'communicating', 'inverters', 'stopped']\n",
      "Topic 17 top words: ['inverter', 'offline', 'blower', 'failurehardware', 'fan', 'fans', 'due', 'replaced', 'replacement', 'hardware']\n",
      "Correlation: 0.155\n",
      "\n",
      "Topic 1 top words: ['inverters', 'faults', 'inverter', 'techdispatched', 'created', 'wo', 'outage', 'central', 'pf', 'b']\n",
      "Topic 29 top words: ['inverter', 'ground', 'inverters', 'fault', 'offline', 'b', 'c', 'created', 'outage', 'techdispatched']\n",
      "Correlation: 0.128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "corr_df = pd.DataFrame(topic_correlations.items())\n",
    "corr_df = corr_df.rename(columns={1:'spearman'})\n",
    "corr_df['topic 1'] = corr_df[0].apply(lambda x: x[0]).astype(int)\n",
    "corr_df['topic 2'] = corr_df[0].apply(lambda x: x[1]).astype(int)\n",
    "corr_df = corr_df.drop(columns=[0])\n",
    "\n",
    "corr_df = corr_df.sort_values('spearman', ascending=False)\n",
    "\n",
    "for _, row in corr_df.head(5).iterrows():\n",
    "    topic_1, topic_2 = int(row['topic 1']), int(row['topic 2'])\n",
    "    print(f'Topic {topic_1} top words:', [term[0] for term in topic_terms[topic_1][:10]])\n",
    "    print(f'Topic {topic_2} top words:', [term[0] for term in topic_terms[topic_2][:10]])\n",
    "    print('Correlation:', round(row['spearman'], 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the results should be taken with a grain of salt, since most documents are only assigned a single topic. However, this example illustrates the process that could be used with a larger number of documents to get more refined topics and hence better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Survival analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.nonparametric import kaplan_meier_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Asset\n",
       "Inverter                    142\n",
       "Central Inverter             17\n",
       "Inverter/String Inverter      2\n",
       "String Inverter               1\n",
       "Inverter module               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CM_df['Asset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function kaplan_meier_estimator in module sksurv.nonparametric:\n",
      "\n",
      "kaplan_meier_estimator(event, time_exit, time_enter=None, time_min=None, reverse=False, conf_level=0.95, conf_type=None)\n",
      "    Kaplan-Meier estimator of survival function.\n",
      "    \n",
      "    See [1]_ for further description.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    event : array-like, shape = (n_samples,)\n",
      "        Contains binary event indicators.\n",
      "    \n",
      "    time_exit : array-like, shape = (n_samples,)\n",
      "        Contains event/censoring times.\n",
      "    \n",
      "    time_enter : array-like, shape = (n_samples,), optional\n",
      "        Contains time when each individual entered the study for\n",
      "        left truncated survival data.\n",
      "    \n",
      "    time_min : float, optional\n",
      "        Compute estimator conditional on survival at least up to\n",
      "        the specified time.\n",
      "    \n",
      "    reverse : bool, optional, default: False\n",
      "        Whether to estimate the censoring distribution.\n",
      "        When there are ties between times at which events are observed,\n",
      "        then events come first and are subtracted from the denominator.\n",
      "        Only available for right-censored data, i.e. `time_enter` must\n",
      "        be None.\n",
      "    \n",
      "    conf_level : float, optional, default: 0.95\n",
      "        The level for a two-sided confidence interval on the survival curves.\n",
      "    \n",
      "    conf_type : None or {'log-log'}, optional, default: None.\n",
      "        The type of confidence intervals to estimate.\n",
      "        If `None`, no confidence intervals are estimated.\n",
      "        If \"log-log\", estimate confidence intervals using\n",
      "        the log hazard or :math:`log(-log(S(t)))` as described in [2]_.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    time : array, shape = (n_times,)\n",
      "        Unique times.\n",
      "    \n",
      "    prob_survival : array, shape = (n_times,)\n",
      "        Survival probability at each unique time point.\n",
      "        If `time_enter` is provided, estimates are conditional probabilities.\n",
      "    \n",
      "    conf_int : array, shape = (2, n_times)\n",
      "        Pointwise confidence interval of the Kaplan-Meier estimator\n",
      "        at each unique time point.\n",
      "        Only provided if `conf_type` is not None.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Creating a Kaplan-Meier curve:\n",
      "    \n",
      "    >>> x, y, conf_int = kaplan_meier_estimator(event, time, conf_type=\"log-log\")\n",
      "    >>> plt.step(x, y, where=\"post\")\n",
      "    >>> plt.fill_between(x, conf_int[0], conf_int[1], alpha=0.25, step=\"post\")\n",
      "    >>> plt.ylim(0, 1)\n",
      "    >>> plt.show()\n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    sksurv.nonparametric.SurvivalFunctionEstimator\n",
      "        Estimator API of the Kaplan-Meier estimator.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Kaplan, E. L. and Meier, P., \"Nonparametric estimation from incomplete observations\",\n",
      "           Journal of The American Statistical Association, vol. 53, pp. 457-481, 1958.\n",
      "    .. [2] Borgan Ø. and Liestøl K., \"A Note on Confidence Intervals and Bands for the\n",
      "           Survival Function Based on Transformations\", Scandinavian Journal of\n",
      "           Statistics. 1990;17(1):35–41.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(kaplan_meier_estimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pvops_bare_with_tweaks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
